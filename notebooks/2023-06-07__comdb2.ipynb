{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf37f20-838f-40f6-bf35-bce010a5c8cf",
   "metadata": {},
   "source": [
    "## comdb2\n",
    "\n",
    "The Data Science Platform supports interacting with various comdb2 tiers. When you launch a job and include a [comdb2 Identity](https://tutti.prod.bloomberg.com/data-science-platform/external_services/comdb2#comdb2-identities), the Platform mounts configuration files directly into your job's environment so that comdb2 and related client libraries can transparently access the appropriate databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b663a3-9ab6-473c-8d66-1db56b278f6d",
   "metadata": {},
   "source": [
    "## Using comdb2 on DSP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c0fc7-62f8-4560-99a2-b72aee2de683",
   "metadata": {},
   "source": [
    "You can use the python-comdb2 client to query comdb2 databases in various tiers. Interactive access to comdb2 is only possible in dev. Non-interactive access can be enabled for any tier when run in an appropriate production cluster in DSP with a production or bridged network policy. [For more details check the comdb2 row in our service accessibility documentation in Tutti](https://tutti.prod.bloomberg.com/data-science-platform/external_services/index#available-services)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2766cab4-0516-4512-ba51-01afc61e7926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from comdb2 import dbapi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a53048-8cfd-4cbc-8b65-77f3784c6d14",
   "metadata": {},
   "source": [
    "### Setting a DB name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eea94886-7d6f-4fda-986b-09fcf2d55b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_name = \"nanadb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c8cba-f648-4be1-8514-befafe9e182a",
   "metadata": {},
   "source": [
    "### Setting a SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33ade7c4-ab2e-4f0a-9c99-77fe0bf16369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from datetime import datetime, date\n",
    "connection = dbapi2.connect(db_name, autocommit=True, tier=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f4a551a-2d98-4022-a759-deca357aca55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setting an appropriate SQL query\n",
    "num_rows_to_fetch = 5000\n",
    "offset = 0\n",
    "fetched_dfs = []\n",
    "date_cutoff = date(2021, 6, 1)\n",
    "\n",
    "sql_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM 'release_type' RT\n",
    "    join release R ON R.release_type = RT.release_type\n",
    "    join story S ON S.release_id = R.id\n",
    "    WHERE wire_id >0 and wire_id NOT IN (25,2345,96,3543,584,474,1719,3447,586,587,97,2640)\n",
    "    AND CAST(R.toa AS DATE) > CAST('2021-01-01' AS DATE)\n",
    "\"\"\"\n",
    "\n",
    "for idx, df in enumerate(pd.read_sql(sql_query, con=connection, chunksize=num_rows_to_fetch)):\n",
    "    df.to_csv(f'one-fetch-{idx}.csv')\n",
    "    if idx % 5 == 0:\n",
    "        print(f'fetched {idx} rows...')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99aacdd0-4afe-4295-b4a3-283403a2d04d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched 0 rows...\n",
      "fetched 5 rows...\n",
      "fetched 10 rows...\n",
      "fetched 15 rows...\n",
      "fetched 20 rows...\n",
      "fetched 25 rows...\n",
      "fetched 30 rows...\n",
      "fetched 35 rows...\n",
      "fetched 40 rows...\n",
      "fetched 45 rows...\n",
      "fetched 50 rows...\n",
      "fetched 55 rows...\n",
      "fetched 60 rows...\n",
      "fetched 65 rows...\n",
      "fetched 70 rows...\n",
      "fetched 75 rows...\n",
      "fetched 80 rows...\n",
      "fetched 85 rows...\n",
      "fetched 90 rows...\n",
      "fetched 95 rows...\n",
      "fetched 100 rows...\n",
      "fetched 105 rows...\n",
      "fetched 110 rows...\n",
      "fetched 115 rows...\n",
      "fetched 120 rows...\n",
      "fetched 125 rows...\n",
      "fetched 130 rows...\n",
      "fetched 135 rows...\n",
      "fetched 140 rows...\n",
      "fetched 145 rows...\n",
      "fetched 150 rows...\n",
      "fetched 155 rows...\n",
      "fetched 160 rows...\n",
      "fetched 165 rows...\n",
      "fetched 170 rows...\n",
      "fetched 175 rows...\n",
      "fetched 180 rows...\n",
      "fetched 185 rows...\n",
      "fetched 190 rows...\n",
      "fetched 195 rows...\n",
      "fetched 200 rows...\n",
      "fetched 205 rows...\n",
      "fetched 210 rows...\n",
      "fetched 215 rows...\n",
      "fetched 220 rows...\n",
      "fetched 225 rows...\n",
      "fetched 230 rows...\n",
      "fetched 235 rows...\n",
      "fetched 240 rows...\n",
      "fetched 245 rows...\n"
     ]
    }
   ],
   "source": [
    "# Setting an appropriate SQL query\n",
    "\n",
    "num_rows_to_fetch = 1000\n",
    "offset = 0\n",
    "fetched_dfs = []\n",
    "date_cutoff = date(2021, 6, 1)\n",
    "\n",
    "idx = 0\n",
    "while True:\n",
    "    sql_query = f\"\"\"\n",
    "        SELECT * FROM 'release_type' RT\n",
    "        join release R on R.release_type = RT.release_type\n",
    "        WHERE wire_id >0 and wire_id NOT IN (25,2345,96,3543,584,474,1719,3447,586,587,97,2640)\n",
    "        ORDER BY toa desc\n",
    "        limit {num_rows_to_fetch}\n",
    "        offset {offset}\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(sql_query, con=connection)\n",
    "    num_records = len(df['toa'].loc[lambda s: s.dt.date > date_cutoff])\n",
    "    if num_records == 0:\n",
    "        break \n",
    "    df.to_csv(f'one-fetch-{idx}.csv')\n",
    "    if idx % 5 == 0:\n",
    "        print(f'fetched {idx} rows...')\n",
    "    offset += num_rows_to_fetch\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "365b3d15-3133-4613-abfd-1ce73123eb91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187b62e-2eb7-41a7-8b9b-3cefed87cfdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "\n",
    "fetched_files = glob.glob('one-fetch-*.csv')\n",
    "fetched_files = sorted(fetched_files, key=lambda x: int(re.search('-(\\d+).csv', x)[1]))\n",
    "\n",
    "processed_dfs = []\n",
    "dump_every = 5\n",
    "proc_num = 0\n",
    "for i, f in tqdm(enumerate(fetched_files)):\n",
    "    df = pd.read_csv(f, index_col=0)\n",
    "    df_proc = (\n",
    "        df\n",
    "         .loc[lambda df: df['language'] == 'en']\n",
    "         .loc[lambda df: df['body'].fillna('').str.strip() != '']\n",
    "         [['toa', 'web_url', 'subject', 'body']]\n",
    "    )\n",
    "\n",
    "    df_proc['processed_body'] = df_proc['body'].apply(lambda x: BeautifulSoup(x).get_text().strip())\n",
    "    df_proc = (df_proc\n",
    "         .drop_duplicates('processed_body')\n",
    "         .drop(columns='body')\n",
    "    )\n",
    "    processed_dfs.append(df_proc)\n",
    "    if i % dump_every == 0:\n",
    "        print('dumping...')\n",
    "        pd.concat(processed_dfs).to_csv(f'processed-nana-{proc_num}.csv')\n",
    "        proc_num += 1\n",
    "        processed_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b391ae-c1bd-43d6-8e64-6abd297b36d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4144f-b9d1-48af-a7af-16c5e54482ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066a2e6-7022-46fc-807d-fb3e6113ab0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b49cbc-2558-4ec0-ba12-c20bfb7b1b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [02:16,  2.72s/it]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas  as pd\n",
    "import re \n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "\n",
    "processed_files = glob.glob('processed-nana-*.csv')\n",
    "processed_files = sorted(processed_files, key=lambda x: int(re.search('-(\\d+).csv', x)[1]))\n",
    "\n",
    "all_processed_dfs = []\n",
    "cutoff = 25\n",
    "num_combined = 0\n",
    "for idx, p_filename in tqdm(enumerate(processed_files)):\n",
    "    all_processed_dfs.append(pd.read_csv(p_filename, index_col=0))\n",
    "    if idx % cutoff == 0:\n",
    "        if len(all_processed_dfs) > 1:\n",
    "            pd.concat(all_processed_dfs).to_csv(\n",
    "                f'big-processed-nana-file-{num_combined}.csv.gz', compression='gzip'\n",
    "            )\n",
    "            num_combined +=1\n",
    "            all_processed_dfs = []\n",
    "            \n",
    "pd.concat(all_processed_dfs).to_csv(\n",
    "    f'big-processed-nana-file-{num_combined}.csv.gz', compression='gzip'\n",
    ")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c4342c8-4a0f-4abd-9558-299962df7718",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_processed_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de66e85-e675-43f6-b505-945c51a4ae3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
