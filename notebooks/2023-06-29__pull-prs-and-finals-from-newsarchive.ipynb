{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "048c245f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T20:48:27.160318Z",
     "start_time": "2023-06-30T20:48:27.158166Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a2c4e7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T20:48:37.445321Z",
     "start_time": "2023-06-30T20:48:37.305992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-01__2022-02-01__release-data.xlsx\r\n",
      "2022-01-01__2022-03-01__suids-rids.xlsx\r\n",
      "2022-03-01__2022-05-01__suids-rids.xlsx\r\n",
      "2022-05-01__2022-07-01__suids-rids.xlsx\r\n",
      "2022-07-01__2022-09-01__suids-rids.xlsx\r\n",
      "2022-09-01__2022-11-01__suids-rids.xlsx\r\n",
      "2022-11-01__2023-02-01__suids-rids.xlsx\r\n",
      "~$2022-01-01__2022-02-01__release-data.xlsx\r\n"
     ]
    }
   ],
   "source": [
    "ls ../data/zomo-downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a6b0721",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T20:51:59.011150Z",
     "start_time": "2023-06-30T20:51:58.896775Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "459e219b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T20:49:29.771213Z",
     "start_time": "2023-06-30T20:49:25.255594Z"
    }
   },
   "outputs": [],
   "source": [
    "first_batch = pd.read_excel('../data/zomo-downloads/2022-01-01__2022-02-01__release-data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef99bab8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T20:51:10.465218Z",
     "start_time": "2023-06-30T20:51:10.458603Z"
    }
   },
   "outputs": [],
   "source": [
    "t = first_batch.loc[lambda df: df['release_body'].str.len() == 10_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "508fe622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T20:52:20.337010Z",
     "start_time": "2023-06-30T20:52:20.323520Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(t['release_body'].iloc[3]).get_text(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d6009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b0b82cd8-be6b-43a6-a991-632fcfa9bc87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains, array_intersect, array, udf, size\n",
    "from pyspark.sql.functions import lit , size, col\n",
    "from pyspark.sql.types import BooleanType\n",
    "import re \n",
    "from bloomberg.ai.librarian import Librarian, get_config\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "def check_overlap(nicodes):\n",
    "    to_exclude = [\n",
    "        'BORGBV',\n",
    "        'BORGFF',\n",
    "        'CMPAUTO',\n",
    "        'BORGDONE',\n",
    "        'BORGSCND',\n",
    "        'BORG2',\n",
    "        'BORG2DON',\n",
    "        'CYBORG',\n",
    "        'HEADS'\n",
    "    ]\n",
    "    \n",
    "    to_include = [\n",
    "        'MGMTCHG', # Management Change\n",
    "        'INI', # Company IPOs\n",
    "        'SECONDBA', # secondary offerings\n",
    "        'ERN', # Earnings cannot be fully automated, because we never know what companies will put in there\n",
    "               # We have extraction to pull non-numerical numbers, itâ€™s not job cuts that we look out for \n",
    "        'BORDONE', #: automated stories with human intervention \n",
    "        'JOBCUTS',  # This is hard, because companies use different wordings to use \n",
    "        'CREDITCG'\n",
    "    ]\n",
    "    \n",
    "    nicode_vals = list(map(lambda x: x.nicode_val, nicodes))\n",
    "    is_excluded = len(set(nicode_vals) & set(to_exclude)) == 0\n",
    "    is_included = len(set(nicode_vals) & set(to_include)) > 0\n",
    "    return is_excluded & is_included\n",
    "    \n",
    "check_overlap_udf = udf(f=check_overlap, returnType=BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1002c24f-22ba-4b35-b9f2-1cc8d76c06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "config.spark.properties[\"spark.executor.instances\"] = 20\n",
    "config.spark.properties[\"spark.executor.memory\"] = \"4G\"\n",
    "config.spark.properties[\"spark.driver.memory\"] = \"6G\"\n",
    "\n",
    "librarian = Librarian(cfg=config)\n",
    "spark = librarian.spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bcacd062-1a4c-4c14-a5bb-0c473054b2ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wires_to_include = [\n",
    "    284, 1883, 12, 39, 1814,\n",
    "    809, 89, 733, 1481, 324,\n",
    "    1883, 1923, 1814, 1925, \n",
    "    1884, 1886, 835, 1924, 921,\n",
    "    1169, 529, 1926, 412,\n",
    "    831, 1897, 1887, 927\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d23d2a-7c2f-4009-a961-5d8883dc22e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/28 16:34:39 WARN kns=aspangher appId=spark-application-1687970047226 execId=driver DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "23/06/28 16:34:49 WARN kns=aspangher appId=spark-application-1687970047226 execId=driver package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "newsarchive_df = (\n",
    "    librarian\n",
    "        .datasets\n",
    "        .newsarchive\n",
    "        .prod()\n",
    "        .pvf_newsldap_4()\n",
    "        .pvf_newsldap_6()\n",
    "        .begin(2023, 4, 1)\n",
    "        .end(2023, 4, 30)\n",
    "    .fetch()\n",
    ")\n",
    "newsarchive_df = newsarchive_df.filter(newsarchive_df.bloom_lang == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3967ee2e-cf0f-412f-a9d2-abf355b227b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "press_release_filtered_df = (\n",
    "    newsarchive_df\n",
    "        .filter(newsarchive_df.wire.isin(wires_to_include))\n",
    "        .select(['suid', 'wire', 'headline', 'body', 'timeofarrival', 'bunch_id'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7c697300-cc3c-42cd-b3d5-819b44c475d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newsarchive_with_press_releases_df = (newsarchive_df\n",
    "   .join(press_release_filtered_df.select('bunch_id'), on='bunch_id', how='left')\n",
    "   .select(['suid', 'wire', 'headline', 'body', 'timeofarrival', 'bunch_id', 'nicodes'])\n",
    ")\n",
    "\n",
    "newsarchive_with_press_releases_df = (\n",
    "    newsarchive_with_press_releases_df\n",
    "        .filter(newsarchive_with_press_releases_df.wire.isin([25, 2345]))\n",
    ")\n",
    "\n",
    "beat_articles_df = (\n",
    "    newsarchive_with_press_releases_df\n",
    "     .filter(check_overlap_udf(newsarchive_with_press_releases_df.nicodes))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fa307e51-2987-465d-8c61-18cc3d758934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['suid', 'wire', 'headline', 'body', 'timeofarrival', 'bunch_id']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "press_release_filtered_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c1080c9b-ece2-4f77-9c8f-7ddf3133acf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['suid', 'wire', 'headline', 'body', 'timeofarrival', 'bunch_id', 'nicodes']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beat_articles_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c8b44198-0968-489f-90e7-dfc7212df21d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joined_df = (\n",
    "    press_release_filtered_df\n",
    "    .withColumnRenamed('suid', 'press_release_suid')\n",
    "    .withColumnRenamed('wire', 'press_release_wire')\n",
    "    .withColumnRenamed('headline', 'press_release_headline')\n",
    "    .withColumnRenamed('body', 'press_release_body')\n",
    "    .withColumnRenamed('timeofarrival', 'press_release_datetime')    \n",
    "    .join(\n",
    "        beat_articles_df\n",
    "            .withColumnRenamed('suid', 'news_article_suid')\n",
    "            .withColumnRenamed('wire', 'news_article_wire')\n",
    "            .withColumnRenamed('headline', 'news_article_headline')\n",
    "            .withColumnRenamed('body', 'news_article_body')\n",
    "            .withColumnRenamed('timeofarrival', 'news_article_datetime')\n",
    "            .withColumnRenamed('nicodes', 'news_article_nicodes')        \n",
    "        , on='bunch_id', how='inner')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8591de8d-fec4-445f-9a4f-366e129eb0e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RemoteIO.ls('hdfs://POB2/user/aspangher/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8669910a-91fa-4d9b-9dbb-b9e63b790901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/29 18:14:36 INFO kns=aspangher appId=spark-application-1687970047226 execId=driver FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/06/29 18:14:36 INFO kns=aspangher appId=spark-application-1687970047226 execId=driver FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/06/29 18:14:36 ERROR kns=aspangher appId=spark-application-1687970047226 execId=driver FileOutputCommitter: Mkdirs failed to create file:/test.json/_temporary/0\n",
      "23/06/29 18:15:04 WARN kns=aspangher appId=spark-application-1687970047226 execId=driver TaskSetManager: Lost task 0.0 in stage 200.0 (TID 11812, 10.208.157.244, executor 11): java.io.IOException: Mkdirs failed to create file:/test.json/_temporary/0/_temporary/attempt_202306291815043152790991228277009_0200_m_000000_11812 (exists=false, cwd=file:/workspace)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:467)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:470)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "23/06/29 18:15:04 WARN kns=aspangher appId=spark-application-1687970047226 execId=driver TaskSetManager: Lost task 0.1 in stage 200.0 (TID 11813, 10.208.152.39, executor 1): java.io.IOException: Mkdirs failed to create file:/test.json/_temporary/0/_temporary/attempt_202306291815046685930125603223246_0200_m_000000_11813 (exists=false, cwd=file:/workspace)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:467)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:470)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "23/06/29 18:15:05 WARN kns=aspangher appId=spark-application-1687970047226 execId=driver TaskSetManager: Lost task 0.2 in stage 200.0 (TID 11814, 10.208.157.254, executor 15): java.io.IOException: Mkdirs failed to create file:/test.json/_temporary/0/_temporary/attempt_20230629181504522853359230950861_0200_m_000000_11814 (exists=false, cwd=file:/workspace)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:467)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:470)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "23/06/29 18:15:05 WARN kns=aspangher appId=spark-application-1687970047226 execId=driver TaskSetManager: Lost task 0.3 in stage 200.0 (TID 11815, 10.208.157.251, executor 13): java.io.IOException: Mkdirs failed to create file:/test.json/_temporary/0/_temporary/attempt_202306291815046863977987715006190_0200_m_000000_11815 (exists=false, cwd=file:/workspace)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:467)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:470)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "23/06/29 18:15:05 ERROR kns=aspangher appId=spark-application-1687970047226 execId=driver TaskSetManager: Task 0 in stage 200.0 failed 4 times; aborting job\n",
      "23/06/29 18:15:05 ERROR kns=aspangher appId=spark-application-1687970047226 execId=driver FileFormatWriter: Aborting job 016e3c58-9110-4cfb-a178-b5eba652d55c.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 200.0 failed 4 times, most recent failure: Lost task 0.3 in stage 200.0 (TID 11815, 10.208.157.251, executor 13): java.io.IOException: Mkdirs failed to create file:/test.json/_temporary/0/_temporary/attempt_202306291815046863977987715006190_0200_m_000000_11815 (exists=false, cwd=file:/workspace)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:467)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:470)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2143)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/test.json/_temporary/0/_temporary/attempt_202306291815046863977987715006190_0200_m_000000_11815 (exists=false, cwd=file:/workspace)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:83)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:467)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:470)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o680.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 200.0 failed 4 times, most recent failure: Lost task 0.3 in stage 200.0 (TID 11815, 10.208.157.251, executor 13): java.io.IOException: Mkdirs failed to create file:/test.json/_temporary/0/_temporary/attempt_202306291815046863977987715006190_0200_m_000000_11815 (exists=false, cwd=file:/workspace)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:83)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:467)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:470)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2143)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\t... 32 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/test.json/_temporary/0/_temporary/attempt_202306291815046863977987715006190_0200_m_000000_11815 (exists=false, cwd=file:/workspace)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:83)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:467)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:470)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_31255/430812893.py\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0moutput_dir\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'file:///test.json'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mcompression\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"none\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m (joined_df\n\u001B[0m\u001B[1;32m      6\u001B[0m      \u001B[0;34m.\u001B[0m\u001B[0mrepartition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartition_count\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m      \u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"overwrite\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/layers/com.bloomberg.ds.buildpacks.spark/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36msave\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    828\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    829\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 830\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    831\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    832\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0msince\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1.4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/layers/com.bloomberg.ds.buildpacks.spark/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/layers/com.bloomberg.ds.buildpacks.spark/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    126\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 128\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    129\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/layers/com.bloomberg.ds.buildpacks.spark/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o680.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 200.0 failed 4 times, most recent failure: Lost task 0.3 in stage 200.0 (TID 11815, 10.208.157.251, executor 13): java.io.IOException: Mkdirs failed to create file:/test.json/_temporary/0/_temporary/attempt_202306291815046863977987715006190_0200_m_000000_11815 (exists=false, cwd=file:/workspace)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:83)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:467)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:470)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2143)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\t... 32 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/test.json/_temporary/0/_temporary/attempt_202306291815046863977987715006190_0200_m_000000_11815 (exists=false, cwd=file:/workspace)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:456)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:441)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:83)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:467)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:470)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "partition_count = 1\n",
    "file_format = 'json'\n",
    "output_dir = 'file:///test.json'\n",
    "compression = \"none\"\n",
    "(joined_df\n",
    "     .repartition(partition_count)\n",
    "     .write.mode(\"overwrite\")\n",
    "     .format(file_format)\n",
    "     .option(\"compression\", compression)\n",
    "     .save(output_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dce2fd-2e84-422c-b487-f6719d450303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704e74f-7530-4e69-960e-2242559496fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7e0f3-7812-4142-bd36-82932f20b979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17c506-8ed1-4437-b504-431263f030e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "37c3ba81-d670-404d-8cd6-2679f8ccee24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6e81cddd-e969-45aa-82ae-3272464211af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 132:====================================================>  (20 + 1) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|    bunch_id|count|\n",
      "+------------+-----+\n",
      "|RTPVSZMB2SK6|    3|\n",
      "|RTRDQ8MB2SJQ|    2|\n",
      "|RSL8LQMB2SJR|    2|\n",
      "|RSXDJNMB2SJK|    1|\n",
      "|RSIP9WMB2SJN|    1|\n",
      "|RSJ7YEMB2SJL|    1|\n",
      "|RSMX7JMB2SJS|    1|\n",
      "|RSXHOTNL4AO2|    1|\n",
      "|RSYCFABLKPOJ|    1|\n",
      "|RTG2DXMB2SJS|    1|\n",
      "|RTRNO1MB2SKX|    1|\n",
      "|RSOPRYMB2SJP|    1|\n",
      "|RT02ZYMB2SJO|    1|\n",
      "|RT3OEMMB2SJN|    1|\n",
      "|RTT99WDWRGG0|    1|\n",
      "|RT0962MB2SJT|    1|\n",
      "|RTBWP0MB2SJN|    1|\n",
      "|RTCMYBMB2SJP|    1|\n",
      "|RTCPSUMB2SK7|    1|\n",
      "|RTT4G2MB2SJZ|    1|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined_df.groupby('bunch_id').count().orderBy('count', ascending=False).limit(1000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "69162a0f-5eaa-4f4a-8525-fe0c6a729e98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    }
   ],
   "source": [
    "joined_pandas_df = (\n",
    "        joined_df\n",
    "        .filter(\n",
    "            joined_df\n",
    "                .bunch_id\n",
    "                .isin(joined_df.groupby('bunch_id')\n",
    "                      .count()\n",
    "                      .orderBy('count', ascending=False)\n",
    "                      .limit(1)\n",
    "                      .bunch_id\n",
    "                     )\n",
    "        )\n",
    "        .limit(1000)\n",
    "        .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aef0525e-6062-4246-a52b-07a49473c4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_df = joined_pandas_df.loc[lambda df: df['bunch_id'] == df['bunch_id'].value_counts().index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac1871-12a7-4eed-a5a3-d2f9393895f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1713107-5b1a-48d4-91bd-ba14d0e4d9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1a872d7b-7204-49bc-8c7d-050f6bd47dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bloomberg.ai.remoteio import RemoteIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7e00eda6-1c6c-48ae-8583-89876063c18e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RemoteIO.ls('hdfs://POB2-GEN/user/aspangher/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c103248d-ca28-4609-9ce2-17a3df80fa08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4c453d-26d9-4c2e-a216-5340fba5f443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8ab8c-90b8-476e-b854-b309ece6c875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75732d63-fc07-4668-b22c-928657a37bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289451b-e73e-431e-9c55-84c8e40b11a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72836310-8311-4ace-8e6d-0378dc190197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15b9d23c-af56-4a51-b330-bad374a97b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "press_releases = newsarchive_with_press_releases_df.limit(200).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04281e-5b8c-4e12-948d-c9435d79a7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8e4aa-4f7f-4eda-9133-9e34acc43ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c20b2b-7569-4e0e-a021-2c328729b1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15e4fa2f-1a31-4bf9-8724-084b89a01e09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bunch_id</th>\n",
       "      <th>suid</th>\n",
       "      <th>wire</th>\n",
       "      <th>headline</th>\n",
       "      <th>body</th>\n",
       "      <th>timeofarrival</th>\n",
       "      <th>nicodes</th>\n",
       "      <th>suid</th>\n",
       "      <th>wire</th>\n",
       "      <th>headline</th>\n",
       "      <th>body</th>\n",
       "      <th>timeofarrival</th>\n",
       "      <th>nicodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RL7AWWT1UM0Y</td>\n",
       "      <td>RL7AWWT1UM0Y</td>\n",
       "      <td>25</td>\n",
       "      <td>Water Theft Proves Lucrative in a Dangerously ...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-text-head...</td>\n",
       "      <td>2023-04-20T13:00:00.003</td>\n",
       "      <td>[(CMD, None, False, False, False, False, 0, No...</td>\n",
       "      <td>RL7AWWT1UM0Y</td>\n",
       "      <td>25</td>\n",
       "      <td>Water Theft Proves Lucrative in a Dangerously ...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-text-head...</td>\n",
       "      <td>2023-04-20T13:00:00.003</td>\n",
       "      <td>[(CMD, None, False, False, False, False, 0, No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RQ001LT1UM0W</td>\n",
       "      <td>RTGJ4AT0G1KX</td>\n",
       "      <td>25</td>\n",
       "      <td>China Central Bank Hints It Will Dial Back Pan...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-text-head...</td>\n",
       "      <td>2023-04-21T09:02:15.535</td>\n",
       "      <td>[(EMTOPZ4, None, False, False, False, False, 4...</td>\n",
       "      <td>RTGJ4AT0G1KX</td>\n",
       "      <td>25</td>\n",
       "      <td>China Central Bank Hints It Will Dial Back Pan...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-text-head...</td>\n",
       "      <td>2023-04-21T09:02:15.535</td>\n",
       "      <td>[(EMTOPZ4, None, False, False, False, False, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RQ001LT1UM0W</td>\n",
       "      <td>RTGJ4AT0G1KX</td>\n",
       "      <td>25</td>\n",
       "      <td>China Central Bank Hints It Will Dial Back Pan...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-text-head...</td>\n",
       "      <td>2023-04-21T09:02:15.535</td>\n",
       "      <td>[(EMTOPZ4, None, False, False, False, False, 4...</td>\n",
       "      <td>RQ001LT1UM0W</td>\n",
       "      <td>25</td>\n",
       "      <td>China Central Bank Signals It Will Dial Back P...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-text-head...</td>\n",
       "      <td>2023-04-21T07:10:25.223</td>\n",
       "      <td>[(ECOCURZ3, None, False, False, False, False, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RQ001LT1UM0W</td>\n",
       "      <td>RQ001LT1UM0W</td>\n",
       "      <td>25</td>\n",
       "      <td>China Central Bank Signals It Will Dial Back P...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-text-head...</td>\n",
       "      <td>2023-04-21T07:10:25.223</td>\n",
       "      <td>[(ECOCURZ3, None, False, False, False, False, ...</td>\n",
       "      <td>RTGJ4AT0G1KX</td>\n",
       "      <td>25</td>\n",
       "      <td>China Central Bank Hints It Will Dial Back Pan...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-text-head...</td>\n",
       "      <td>2023-04-21T09:02:15.535</td>\n",
       "      <td>[(EMTOPZ4, None, False, False, False, False, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RQ001LT1UM0W</td>\n",
       "      <td>RQ001LT1UM0W</td>\n",
       "      <td>25</td>\n",
       "      <td>China Central Bank Signals It Will Dial Back P...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-text-head...</td>\n",
       "      <td>2023-04-21T07:10:25.223</td>\n",
       "      <td>[(ECOCURZ3, None, False, False, False, False, ...</td>\n",
       "      <td>RQ001LT1UM0W</td>\n",
       "      <td>25</td>\n",
       "      <td>China Central Bank Signals It Will Dial Back P...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-text-head...</td>\n",
       "      <td>2023-04-21T07:10:25.223</td>\n",
       "      <td>[(ECOCURZ3, None, False, False, False, False, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>RT0V2XDWX2PS</td>\n",
       "      <td>RT242YDWRGG3</td>\n",
       "      <td>25</td>\n",
       "      <td>Treasury Investors Ready for a Slow Grinding R...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-abstract\"...</td>\n",
       "      <td>2023-04-13T13:56:58.864</td>\n",
       "      <td>[(GLOBFEAT, None, False, False, False, False, ...</td>\n",
       "      <td>RT1CWET1UM13</td>\n",
       "      <td>25</td>\n",
       "      <td>China's Big Trade Surprise Cushions Tech Weakn...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-abstract\"...</td>\n",
       "      <td>2023-04-13T04:09:50.762</td>\n",
       "      <td>[(GLOBFEAT, None, False, False, False, False, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>RT0V2XDWX2PS</td>\n",
       "      <td>RT242YDWRGG3</td>\n",
       "      <td>25</td>\n",
       "      <td>Treasury Investors Ready for a Slow Grinding R...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-abstract\"...</td>\n",
       "      <td>2023-04-13T13:56:58.864</td>\n",
       "      <td>[(GLOBFEAT, None, False, False, False, False, ...</td>\n",
       "      <td>RT2H7KDWX2PS</td>\n",
       "      <td>25</td>\n",
       "      <td>JPMorgan Back-to-Office Call Shows Real Estate...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-abstract\"...</td>\n",
       "      <td>2023-04-13T18:40:32.058</td>\n",
       "      <td>[(GLOBFEAT, None, False, False, False, False, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>RT0V2XDWX2PS</td>\n",
       "      <td>RT242YDWRGG3</td>\n",
       "      <td>25</td>\n",
       "      <td>Treasury Investors Ready for a Slow Grinding R...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-abstract\"...</td>\n",
       "      <td>2023-04-13T13:56:58.864</td>\n",
       "      <td>[(GLOBFEAT, None, False, False, False, False, ...</td>\n",
       "      <td>RT208YDWX2PW</td>\n",
       "      <td>25</td>\n",
       "      <td>Treasury Yields Test Session Lows on US PPI Da...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-abstract\"...</td>\n",
       "      <td>2023-04-13T12:34:10.585</td>\n",
       "      <td>[(GLOBFEAT, None, False, False, False, False, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>RT0V2XDWX2PS</td>\n",
       "      <td>RT242YDWRGG3</td>\n",
       "      <td>25</td>\n",
       "      <td>Treasury Investors Ready for a Slow Grinding R...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-abstract\"...</td>\n",
       "      <td>2023-04-13T13:56:58.864</td>\n",
       "      <td>[(GLOBFEAT, None, False, False, False, False, ...</td>\n",
       "      <td>RT20W8DWRGG1</td>\n",
       "      <td>25</td>\n",
       "      <td>Dollar Approaches Key Area Signaling More Decl...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-abstract\"...</td>\n",
       "      <td>2023-04-13T12:48:08.737</td>\n",
       "      <td>[(GLOBFEAT, None, False, False, False, False, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>RT0V2XDWX2PS</td>\n",
       "      <td>RT242YDWRGG3</td>\n",
       "      <td>25</td>\n",
       "      <td>Treasury Investors Ready for a Slow Grinding R...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-abstract\"...</td>\n",
       "      <td>2023-04-13T13:56:58.864</td>\n",
       "      <td>[(GLOBFEAT, None, False, False, False, False, ...</td>\n",
       "      <td>RT19DCDWX2PT</td>\n",
       "      <td>25</td>\n",
       "      <td>Iron Ore Faces a Test as Outlook in China Dark...</td>\n",
       "      <td>&lt;sections&gt;\\n&lt;section class=\"news-rsf-abstract\"...</td>\n",
       "      <td>2023-04-13T02:53:36.455</td>\n",
       "      <td>[(GLOBFEAT, None, False, False, False, False, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         bunch_id          suid  wire  \\\n",
       "0    RL7AWWT1UM0Y  RL7AWWT1UM0Y    25   \n",
       "1    RQ001LT1UM0W  RTGJ4AT0G1KX    25   \n",
       "2    RQ001LT1UM0W  RTGJ4AT0G1KX    25   \n",
       "3    RQ001LT1UM0W  RQ001LT1UM0W    25   \n",
       "4    RQ001LT1UM0W  RQ001LT1UM0W    25   \n",
       "..            ...           ...   ...   \n",
       "995  RT0V2XDWX2PS  RT242YDWRGG3    25   \n",
       "996  RT0V2XDWX2PS  RT242YDWRGG3    25   \n",
       "997  RT0V2XDWX2PS  RT242YDWRGG3    25   \n",
       "998  RT0V2XDWX2PS  RT242YDWRGG3    25   \n",
       "999  RT0V2XDWX2PS  RT242YDWRGG3    25   \n",
       "\n",
       "                                              headline  \\\n",
       "0    Water Theft Proves Lucrative in a Dangerously ...   \n",
       "1    China Central Bank Hints It Will Dial Back Pan...   \n",
       "2    China Central Bank Hints It Will Dial Back Pan...   \n",
       "3    China Central Bank Signals It Will Dial Back P...   \n",
       "4    China Central Bank Signals It Will Dial Back P...   \n",
       "..                                                 ...   \n",
       "995  Treasury Investors Ready for a Slow Grinding R...   \n",
       "996  Treasury Investors Ready for a Slow Grinding R...   \n",
       "997  Treasury Investors Ready for a Slow Grinding R...   \n",
       "998  Treasury Investors Ready for a Slow Grinding R...   \n",
       "999  Treasury Investors Ready for a Slow Grinding R...   \n",
       "\n",
       "                                                  body  \\\n",
       "0    <sections>\\n<section class=\"news-rsf-text-head...   \n",
       "1    <sections>\\n<section class=\"news-rsf-text-head...   \n",
       "2    <sections>\\n<section class=\"news-rsf-text-head...   \n",
       "3    <sections>\\n<section class=\"news-rsf-text-head...   \n",
       "4    <sections>\\n<section class=\"news-rsf-text-head...   \n",
       "..                                                 ...   \n",
       "995  <sections>\\n<section class=\"news-rsf-abstract\"...   \n",
       "996  <sections>\\n<section class=\"news-rsf-abstract\"...   \n",
       "997  <sections>\\n<section class=\"news-rsf-abstract\"...   \n",
       "998  <sections>\\n<section class=\"news-rsf-abstract\"...   \n",
       "999  <sections>\\n<section class=\"news-rsf-abstract\"...   \n",
       "\n",
       "               timeofarrival  \\\n",
       "0    2023-04-20T13:00:00.003   \n",
       "1    2023-04-21T09:02:15.535   \n",
       "2    2023-04-21T09:02:15.535   \n",
       "3    2023-04-21T07:10:25.223   \n",
       "4    2023-04-21T07:10:25.223   \n",
       "..                       ...   \n",
       "995  2023-04-13T13:56:58.864   \n",
       "996  2023-04-13T13:56:58.864   \n",
       "997  2023-04-13T13:56:58.864   \n",
       "998  2023-04-13T13:56:58.864   \n",
       "999  2023-04-13T13:56:58.864   \n",
       "\n",
       "                                               nicodes          suid  wire  \\\n",
       "0    [(CMD, None, False, False, False, False, 0, No...  RL7AWWT1UM0Y    25   \n",
       "1    [(EMTOPZ4, None, False, False, False, False, 4...  RTGJ4AT0G1KX    25   \n",
       "2    [(EMTOPZ4, None, False, False, False, False, 4...  RQ001LT1UM0W    25   \n",
       "3    [(ECOCURZ3, None, False, False, False, False, ...  RTGJ4AT0G1KX    25   \n",
       "4    [(ECOCURZ3, None, False, False, False, False, ...  RQ001LT1UM0W    25   \n",
       "..                                                 ...           ...   ...   \n",
       "995  [(GLOBFEAT, None, False, False, False, False, ...  RT1CWET1UM13    25   \n",
       "996  [(GLOBFEAT, None, False, False, False, False, ...  RT2H7KDWX2PS    25   \n",
       "997  [(GLOBFEAT, None, False, False, False, False, ...  RT208YDWX2PW    25   \n",
       "998  [(GLOBFEAT, None, False, False, False, False, ...  RT20W8DWRGG1    25   \n",
       "999  [(GLOBFEAT, None, False, False, False, False, ...  RT19DCDWX2PT    25   \n",
       "\n",
       "                                              headline  \\\n",
       "0    Water Theft Proves Lucrative in a Dangerously ...   \n",
       "1    China Central Bank Hints It Will Dial Back Pan...   \n",
       "2    China Central Bank Signals It Will Dial Back P...   \n",
       "3    China Central Bank Hints It Will Dial Back Pan...   \n",
       "4    China Central Bank Signals It Will Dial Back P...   \n",
       "..                                                 ...   \n",
       "995  China's Big Trade Surprise Cushions Tech Weakn...   \n",
       "996  JPMorgan Back-to-Office Call Shows Real Estate...   \n",
       "997  Treasury Yields Test Session Lows on US PPI Da...   \n",
       "998  Dollar Approaches Key Area Signaling More Decl...   \n",
       "999  Iron Ore Faces a Test as Outlook in China Dark...   \n",
       "\n",
       "                                                  body  \\\n",
       "0    <sections>\\n<section class=\"news-rsf-text-head...   \n",
       "1    <sections>\\n<section class=\"news-rsf-text-head...   \n",
       "2    <sections>\\n<section class=\"news-rsf-text-head...   \n",
       "3    <sections>\\n<section class=\"news-rsf-text-head...   \n",
       "4    <sections>\\n<section class=\"news-rsf-text-head...   \n",
       "..                                                 ...   \n",
       "995  <sections>\\n<section class=\"news-rsf-abstract\"...   \n",
       "996  <sections>\\n<section class=\"news-rsf-abstract\"...   \n",
       "997  <sections>\\n<section class=\"news-rsf-abstract\"...   \n",
       "998  <sections>\\n<section class=\"news-rsf-abstract\"...   \n",
       "999  <sections>\\n<section class=\"news-rsf-abstract\"...   \n",
       "\n",
       "               timeofarrival  \\\n",
       "0    2023-04-20T13:00:00.003   \n",
       "1    2023-04-21T09:02:15.535   \n",
       "2    2023-04-21T07:10:25.223   \n",
       "3    2023-04-21T09:02:15.535   \n",
       "4    2023-04-21T07:10:25.223   \n",
       "..                       ...   \n",
       "995  2023-04-13T04:09:50.762   \n",
       "996  2023-04-13T18:40:32.058   \n",
       "997  2023-04-13T12:34:10.585   \n",
       "998  2023-04-13T12:48:08.737   \n",
       "999  2023-04-13T02:53:36.455   \n",
       "\n",
       "                                               nicodes  \n",
       "0    [(CMD, None, False, False, False, False, 0, No...  \n",
       "1    [(EMTOPZ4, None, False, False, False, False, 4...  \n",
       "2    [(ECOCURZ3, None, False, False, False, False, ...  \n",
       "3    [(EMTOPZ4, None, False, False, False, False, 4...  \n",
       "4    [(ECOCURZ3, None, False, False, False, False, ...  \n",
       "..                                                 ...  \n",
       "995  [(GLOBFEAT, None, False, False, False, False, ...  \n",
       "996  [(GLOBFEAT, None, False, False, False, False, ...  \n",
       "997  [(GLOBFEAT, None, False, False, False, False, ...  \n",
       "998  [(GLOBFEAT, None, False, False, False, False, ...  \n",
       "999  [(GLOBFEAT, None, False, False, False, False, ...  \n",
       "\n",
       "[1000 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6422e7d2-19d9-464e-8e3e-ef41bd4f360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_wires = [\n",
    "    'SPC', # Wire for S&P releases\n",
    "    'FII', # Fitch releases \n",
    "    'EDG', # \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "932aa644-c30f-4f72-b42b-c303b86ae70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wire_code</th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>location</th>\n",
       "      <th>full_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>284</td>\n",
       "      <td>EDG</td>\n",
       "      <td>FILINGS</td>\n",
       "      <td>Edgar SEC-Online</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wire_code abbreviation location         full_name\n",
       "5        284          EDG  FILINGS  Edgar SEC-Online"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wire_code_df.loc[lambda df: df['abbreviation'].isin(additional_wires)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f8bfec-b774-453e-b8d0-da539f0e6fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89778777-c866-4cf5-bd60-e3aa8715626e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dcc777-315d-4a2e-8945-f32b6e15e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "wire_code_df = pd.read_csv('wire-codes-with-columns.csv', index_col=0)\n",
    "_ = wire_code_df.loc[lambda df: df['full_name'].str.contains('First Word')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf264f-3c71-47f0-ab87-58d1ead807cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NWSKAFNQ     100\n",
       "BNALL        100\n",
       "2500         100\n",
       "BIZNEWS       98\n",
       "FINNEWS       96\n",
       "BUSINESS      96\n",
       "READ25        95\n",
       "NCAS          91\n",
       "READ          91\n",
       "READ50        91\n",
       "ALLTOP        90\n",
       "READ100       83\n",
       "MSCIWORLD     81\n",
       "WWTOP         78\n",
       "READ150       77\n",
       "ONWEB         72\n",
       "WWTOPFEAT     72\n",
       "COS           69\n",
       "READ250       66\n",
       "TOP           64\n",
       "dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = (newsarchive_with_press_releases_df\n",
    "     .filter(check_overlap_udf(newsarchive_with_press_releases_df.nicodes))\n",
    "     .limit(100)\n",
    "     .toPandas()\n",
    ")\n",
    "(t['nicodes']\n",
    " .apply(lambda x: list(map(lambda y: y.nicode_val, x)))\n",
    " .str.join(' ')\n",
    " .str.split(' ', expand=True)\n",
    " .unstack().dropna().value_counts()\n",
    " .head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "22940610-45e7-4547-b85d-25d05af6a2ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a20aeafc-acac-4ad4-973d-fe09b5c276bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(t['body'].iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4c86cd9c-438e-4ace-b07d-ed1cff5afa2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank Turmoil Seen Crimping Credit at Double Powell's Estimate \n",
      " Bank Turmoil Seen Crimping Credit at Double Powell's Estimate \n",
      " Most economists see half-point hike impact or more from banks Consensus sees recession coming, likely this quarter or next \n",
      " By Steve Matthews and Sarina Yoo (Bloomberg) -- \n",
      " US bank stress will tighten credit by twice as much as expected by Federal Reserve Chair Jerome Powell , said economists surveyed by Bloomberg, tipping the economy into recession. Almost all of the economists expect the Federal Open Market Committee to hike interest rates another quarter percentage point at its May 2-3 meeting, to a target range of 5% to 5.25%. But the higher borrowing costs will be amplified by the fallout from the March collapse of two US banks, which a majority of the economists found to be equivalent to a Fed hike of about half a percentage point or more. Powell has estimated the impact at roughly a quarter point. \"Inflation remains unacceptably high, but banking stresses are leading to a tightening of lending conditions and this will do more to slow the economy than the likely 25 basis-point hike on Wednesday,\" said James Knightley , chief international economist at ING, in a survey response. The survey of 46 economists was conducted April 21-26. Fed officials have mostly downplayed the impact on monetary policy from the failures of Silicon Valley Bank and Signature Bank, though they have expressed uncertainly over how evolving credit conditions will affect growth the remainder of the year. In addition to those two failures, First Republic Bank shares plunged this week after encountering huge outflows of deposits. The Fed has made emergency loans to banks and authorities have guaranteed deposits in excess of stated limits to try to stem the crisis. A majority of the economists say the banking woes are mostly over, while another quarter say the crisis is about half over. What Bloomberg Economics Says \"It would take a durable banking-sector shock to cause the Fed to materially slow rate hikes. In our view, the current shock is likely to subside sometime in 2Q, and the adverse impact will replace about one 25-basis point Fed hike.\" -- Anna Wong , chief US economist. For full analysis, click here The resulting credit impact will be significant, the economists found. Almost all of the economists say banks will tighten lending standards, with the biggest impact on commercial real estate loans. A majority see lending standards on real estate loans tightening somewhat and 41% see a considerable impact. Nearly three quarters of the economists expect substantial losses in the office sector. \"We expect deposit outflows to continue,\" Nomura Securities economists Aichi Amemiya and Jacob Meyer said. \"This is likely to be a key cause of reduced bank lending in the coming quarters, which we expect to generate notable economic headwinds.\" Views on the impact on monetary policy are diverse, with 43% of the economists estimating it as equivalent to a half-point hike, another 13% seeing it amounting to between a 75 basis-point and 150 basis-point hike, while a quarter agree with Powell in seeing it as a quarter-point impact. The Fed chair, in his press conference in March, emphasized there's quite a bit of uncertainty about his estimate: \"You can think of it as being the equivalent of a rate hike or perhaps more than that; of course, it's not possible to make that assessment today with any precision whatsoever,\" he said. The exact impact may not be easy to calibrate and can be unpredictable, added Julia Coronado , president of MacroPolicy Perspectives LLC and a former Fed economist. \"Thinking in terms of rate hike equivalents leads us to think about monetary policy in a dangerously linear way,\" she said. \"The faster the tightening, the greater the risk of nonlinear financial stability events that can lead to a nonlinear tightening in credit.\" The Fed has engaged in the most aggressive interest-rate hiking in 40 years to try to fight persistently high inflation. The upshot: The hiking and the banking crisis will tip the economy into recession within the next 12 months, according to two thirds of the economists. Among those who expect a downturn, four fifths expect it to start in the current quarter or next quarter. \"By outsourcing the rest of the fight against inflation to credit tightening by the banks, it has become even more difficult for the Fed to engineer a soft landing,\" said Philip Marey , senior US strategist at Rabobank. \"Consequently, we think a recession has become more likely.\" Read More: Housing-Market Bottom Raises Hopes That US Can Avoid Recession While FOMC officials haven't explicitly predicted a recession, the Fed staff at the last meeting forecast a mild recession and policy makers' forecasts made in March suggest a sharp reduction in growth this year. Half the economists blame the poor oversight on the collapsing banks on the Fed's supervisory staff and its failure to act promptly in response to institutions' weaknesses, while a third say the tailoring of regulation by Congress that resulted in lighter supervision of midsized institutions was principally to blame. The Fed plans to release its internal review of the supervision and regulation of failed lender Silicon Valley Bank on Friday at 11 a.m. New York time. \n",
      " \n",
      " To contact the reporters on this story: Steve Matthews in Atlanta at smatthews@bloomberg.net ; Sarina Yoo in New York at kyoo3@bloomberg.net To contact the editors responsible for this story: Alister Bull at abull7@bloomberg.net Robert Jameson \n",
      " Attribution Steve Matthews : by reporter Sarina Yoo : by reporter Alister Bull : editor primary responsible Robert Jameson : editor secondary\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(' +', ' ', unidecode(soup.get_text(' ').strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f103a7-e71e-4a20-9a46-25c5679cbcc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851770a-3c18-49e5-8ae5-0bffb435ea0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e6a23-062e-4a00-a143-e3a9fcb35a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb65c42-a217-449f-b9aa-255c2b3b9a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9876e480-7d21-4a72-bc15-83cb3d236651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bunch_ids_with_at_least_two_rows = (\n",
    "    newsarchive_df\n",
    "        .join(filtered_df.select('bunch_id'), on='bunch_id', how='left')\n",
    "        .distinct()\n",
    "        .groupBy(\"bunch_id\")\n",
    "        .count()\n",
    "        .filter(\"`count` >= 2\")\n",
    "        .select([\"bunch_id\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38ce01d3-72ee-4ee1-ab72-9656e8403284",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bunch_ids_with_at_least_two_rows = bunch_ids_with_at_least_two_rows.rdd.map(lambda x :  x[0]).collect()\n",
    "# print(f\"len(bunch_ids_with_at_least_two_rows) = {len(bunch_ids_with_at_least_two_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49c9f65f-8b94-4a05-abef-47e5e4095da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute '_get_object_id'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_45080/2865099010.py\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m df_filted_by_bunch_id = (\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mnewsarchive_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnewsarchive_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbunch_id\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbunch_ids_with_at_least_two_rows\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m         \u001B[0;34m.\u001B[0m\u001B[0msort\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'bunch_id'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'suid'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'wire'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'headline'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'body'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'bloom_lang'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'timeofarrival'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'bunch_id'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m )\n",
      "\u001B[0;32m/layers/com.bloomberg.ds.buildpacks.spark/spark/python/pyspark/sql/column.py\u001B[0m in \u001B[0;36misin\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m    450\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    451\u001B[0m             \u001B[0mcols\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 452\u001B[0;31m         \u001B[0mcols\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0m_create_column_from_literal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mc\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    453\u001B[0m         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    454\u001B[0m         \u001B[0mjc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"isin\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/layers/com.bloomberg.ds.buildpacks.spark/spark/python/pyspark/sql/column.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    450\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    451\u001B[0m             \u001B[0mcols\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 452\u001B[0;31m         \u001B[0mcols\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0m_create_column_from_literal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mc\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    453\u001B[0m         \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    454\u001B[0m         \u001B[0mjc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"isin\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/layers/com.bloomberg.ds.buildpacks.spark/spark/python/pyspark/sql/column.py\u001B[0m in \u001B[0;36m_create_column_from_literal\u001B[0;34m(literal)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_create_column_from_literal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mliteral\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m     \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mliteral\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/layers/com.bloomberg.ds.buildpacks.spark/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1294\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1295\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1296\u001B[0;31m         \u001B[0margs_command\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_build_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1297\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1298\u001B[0m         \u001B[0mcommand\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mproto\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCALL_COMMAND_NAME\u001B[0m \u001B[0;34m+\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/layers/com.bloomberg.ds.buildpacks.spark/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m_build_args\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1264\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1265\u001B[0m         args_command = \"\".join(\n\u001B[0;32m-> 1266\u001B[0;31m             [get_command_part(arg, self.pool) for arg in new_args])\n\u001B[0m\u001B[1;32m   1267\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1268\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0margs_command\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/layers/com.bloomberg.ds.buildpacks.spark/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1264\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1265\u001B[0m         args_command = \"\".join(\n\u001B[0;32m-> 1266\u001B[0;31m             [get_command_part(arg, self.pool) for arg in new_args])\n\u001B[0m\u001B[1;32m   1267\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1268\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0margs_command\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/layers/com.bloomberg.ds.buildpacks.spark/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_command_part\u001B[0;34m(parameter, python_proxy_pool)\u001B[0m\n\u001B[1;32m    296\u001B[0m             \u001B[0mcommand_part\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;34m\";\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0minterface\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    297\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 298\u001B[0;31m         \u001B[0mcommand_part\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mparameter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_object_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    299\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    300\u001B[0m     \u001B[0mcommand_part\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;34m\"\\n\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'PipelinedRDD' object has no attribute '_get_object_id'"
     ]
    }
   ],
   "source": [
    "df_filted_by_bunch_id = (\n",
    "    newsarchive_df.filter(newsarchive_df.bunch_id.isin(bunch_ids_with_at_least_two_rows))\n",
    "        .sort('bunch_id')\n",
    "        .select(['suid', 'wire', 'headline', 'body', 'bloom_lang', 'timeofarrival', 'bunch_id'])\n",
    ")\n",
    "# df_filted_by_bunch_id = df_filted_by_bunch_id.filter(df_filted_by_bunch_id.wire.isin("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e400fd5-77a1-4d7d-8621-bd487155b1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filted_by_bunch_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c731d79-5b00-44f4-993d-c5d72c9ff246",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:===================================================>(4470 + 3) / 4473]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+----+--------------------+----+----------+--------------------+------------+\n",
      "|        suid|wire_mnemonic|wire|            headline|body|bloom_lang|       timeofarrival|    bunch_id|\n",
      "+------------+-------------+----+--------------------+----+----------+--------------------+------------+\n",
      "|QMJGLKMB2SJQ|          GO1|1883|FDA: Abbreviated ...|    |         1|2021-01-07T00:41:...|QMJGLKMB2SJQ|\n",
      "|QMJGLKMB2SJQ|          GO1|1883|FDA: Abbreviated ...|    |         1|2021-01-07T00:41:...|QMJGLKMB2SJQ|\n",
      "|QMJU2BMB2SJW|          GO6|1884|India Res Bank: U...|    |         1|2021-01-07T05:32:...|QMJU2BMB2SJW|\n",
      "|QMJU2BMB2SJW|          GO6|1884|India Res Bank: U...|    |         1|2021-01-07T05:32:...|QMJU2BMB2SJW|\n",
      "|QMJY46MB2SJU|          GO5|1886|Germany Stats: Pr...|    |         1|2021-01-07T07:00:...|QMJY46MB2SJU|\n",
      "|QMJY46MB2SJU|          GO5|1886|Germany Stats: Pr...|    |         1|2021-01-07T07:00:...|QMJY46MB2SJU|\n",
      "|QMK0XCMB2SJZ|          GO5|1886|Stats Austria: Wh...|    |         1|2021-01-07T08:00:...|QMK0XCMB2SJZ|\n",
      "|QMK0XCMB2SJZ|          GO5|1886|Stats Austria: Wh...|    |         1|2021-01-07T08:00:...|QMK0XCMB2SJZ|\n",
      "|QMK42TMB2SJW|          GO6|1884|India Res Bank: S...|    |         1|2021-01-07T09:08:...|QMK42TMB2SJW|\n",
      "|QMK42TMB2SJW|          GO6|1884|India Res Bank: S...|    |         1|2021-01-07T09:08:...|QMK42TMB2SJW|\n",
      "|QMK6SJMB2SJN|          GO6|1884|India Res Bank: R...|    |         1|2021-01-07T10:07:...|QMK6SJMB2SJN|\n",
      "|QMK6SJMB2SJN|          GO6|1884|India Res Bank: R...|    |         1|2021-01-07T10:07:...|QMK6SJMB2SJN|\n",
      "|QMK9IZMB2SJS|          GO1|1883|NHTSA: RECALL Eng...|    |         1|2021-01-07T11:06:...|QMK9IZMB2SJS|\n",
      "|QMK9IZMB2SJS|          GO1|1883|NHTSA: RECALL Eng...|    |         1|2021-01-07T11:06:...|QMK9IZMB2SJS|\n",
      "|QMKACCMB2SJS|          GO6|1884|India Res Bank: R...|    |         1|2021-01-07T11:24:...|QMKACCMB2SJS|\n",
      "|QMKACCMB2SJS|          GO6|1884|India Res Bank: R...|    |         1|2021-01-07T11:24:...|QMKACCMB2SJS|\n",
      "|QMKG60MB2SJL|          GO1|1883|US Census Bur: U....|    |         1|2021-01-07T13:30:...|QMKG60MB2SJL|\n",
      "|QMKG60MB2SJL|          GO1|1883|US Census Bur: U....|    |         1|2021-01-07T13:30:...|QMKG60MB2SJL|\n",
      "|QMKG61MB2SJW|          GO1|1883|US Census Bur: In...|    |         1|2021-01-07T13:30:...|QMKG61MB2SJW|\n",
      "|QMKG61MB2SJW|          GO1|1883|US Census Bur: In...|    |         1|2021-01-07T13:30:...|QMKG61MB2SJW|\n",
      "+------------+-------------+----+--------------------+----+----------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_filted_by_bunch_id.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78ea97-0b19-4af6-a087-0026405c8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df_filted_by_bunch_id.withColumn(\"row_number\",F.row_number().over(Window.partitionBy(df_filted_by_bunch_id.headline).orderBy(df_filted_by_bunch_id.headline.desc()))).filter(F.col(\"row_number\")==1).drop(\"row_number\").sort(\"bunch_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f46fab7c-7111-4eac-8e5b-6945e4bf3412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f0086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384737b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a2f0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312612ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c620c81",
   "metadata": {},
   "source": [
    "# Format newsdiscourse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "039b5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsdiscourse_df = (\n",
    "    pd.read_csv('../models/discourse-model/data/news-discourse-training-data.csv')\n",
    "        .assign(s_id=lambda df: df['s_id'].str.replace('S', '').astype(int))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45412b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsdiscourse_df = (\n",
    "    newsdiscourse_df\n",
    "     .sort_values(['name', 's_id'])\n",
    "     .assign(split=lambda df: df['file'].str.split('/').str.get(4))\n",
    "     .assign(event_tag=lambda df: df['event_tag'].fillna('Error'))\n",
    "     .groupby('name')\n",
    "     [['sentence', 'event_tag', 'split']]\n",
    "     .aggregate(list)\n",
    "     .assign(split=lambda df: df['split'].str.get(0) )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d658be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(newsdiscourse_df\n",
    " .to_json(\n",
    "     path_or_buf='../models/discourse-model/data/news-discourse-data.jsonl',\n",
    "     orient='records',\n",
    "     lines=True\n",
    " )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b5843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b982eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ecbf9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
